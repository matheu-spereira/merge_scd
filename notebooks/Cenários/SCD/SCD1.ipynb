{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8d18b6-f6c9-4284-8fae-36ba273c203d",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cae8165-c7f8-4ba3-8c52-5dc0138d1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "import os\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e55ec6-c596-451b-b221-de7676d06b9a",
   "metadata": {},
   "source": [
    "# Sessão SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ec1fd2-98d3-4f9c-9faa-bb31266a7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .master(\"spark://spark-master:7077\") \\\n",
    "            .config(\"spark.jars.packages\", \n",
    "                    \"org.apache.hadoop:hadoop-aws:3.2.2,\"\n",
    "                    \"io.delta:delta-spark_2.12:3.2.0,\"\n",
    "                    \"io.delta:delta-storage:3.2.0,\"\n",
    "                    \"com.amazonaws:aws-java-sdk-bundle:1.12.180\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.executor.cores\", \"2\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.driver.cores\", \"2\") \\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "            .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
    "            .config(\"spark.dynamicAllocation.maxExecutors\", \"2\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "\t        .config(\"spark.hadoop.fs.s3a.endpoint\", os.getenv(\"MINIO_HOST\")) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"MINIO_ACCESS_KEY\")) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"MINIO_SECRET_KEY\")) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "            .config(\"spark.hadoop.com.amazonaws.services.s3.enableV4\", \"true\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "            .config(\"spark.hadoop.fs.AbstractFileSystem.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3A\") \\\n",
    "            .getOrCreate()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ec9a8-5c12-43f5-a14a-a1afae87feea",
   "metadata": {},
   "source": [
    "# Tabela Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57716f75-1404-49e0-b644-720aa07f8cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo o caminho do MinIO para armazenar os dados no formato Delta\n",
    "emp_path = \"s3a://silver/employe\"\n",
    "\n",
    "# Criando a tabela de destino 'employe' no formato Delta\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS employe (\n",
    "    emp_id INT,\n",
    "    emp_name STRING,\n",
    "    dept_code STRING,\n",
    "    salary DOUBLE\n",
    ") USING DELTA LOCATION '{emp_path}'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Inserindo valores fictícios na tabela employe\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO employe (emp_id, emp_name, dept_code, salary)\n",
    "VALUES \n",
    "    (1001, 'Alice', 'D101', 55000),\n",
    "    (1002, 'Bob', 'D102', 60000),\n",
    "    (1003, 'Charlie', 'D103', 75000),\n",
    "    (1004, 'David', 'D104', 65000),\n",
    "    (1005, 'Eve', 'D105', 70000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89ee605-e76c-45e2-a2fe-5559847f2b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+\n",
      "|emp_id|emp_name|dept_code| salary|\n",
      "+------+--------+---------+-------+\n",
      "|  1003| Charlie|     D103|75000.0|\n",
      "|  1004|   David|     D104|65000.0|\n",
      "|  1005|     Eve|     D105|70000.0|\n",
      "|  1001|   Alice|     D101|55000.0|\n",
      "|  1002|     Bob|     D102|60000.0|\n",
      "+------+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employe\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490dc2cf-701f-4d1f-9937-2897e94c4785",
   "metadata": {},
   "source": [
    "# Tabela Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6b3e25-95bf-4334-8a71-eeac72d434dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates_path = \"s3a://bronze/employe\"\n",
    "\n",
    "# Criando a tabela de origem 'employe_update' no formato Delta\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS employe_update (\n",
    "    emp_id INT,\n",
    "    emp_name STRING,\n",
    "    dept_code STRING,\n",
    "    salary DOUBLE\n",
    ") USING DELTA LOCATION '{updates_path}'\n",
    "\"\"\")\n",
    "\n",
    "# Inserindo valores fictícios na tabela employe_update\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO employe_update (emp_id, emp_name, dept_code, salary)\n",
    "VALUES \n",
    "    (1001, 'Alice', 'D101', 58000),  -- Atualização do salário de Alice\n",
    "    (1002, 'Bob', 'D102', 62000),    -- Atualização do salário de Bob\n",
    "    (1006, 'Frank', 'D106', 70000),  -- Novo funcionário (não existe na tabela de emp)\n",
    "    (1007, 'Grace', 'D107', 75000)   -- Novo funcionário (não existe na tabela de emp)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e83b12-6a98-4deb-bd4a-81f8ca7553bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+\n",
      "|emp_id|emp_name|dept_code| salary|\n",
      "+------+--------+---------+-------+\n",
      "|  1007|   Grace|     D107|75000.0|\n",
      "|  1006|   Frank|     D106|70000.0|\n",
      "|  1001|   Alice|     D101|58000.0|\n",
      "|  1002|     Bob|     D102|62000.0|\n",
      "+------+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employe_update\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e50f0-89f3-4309-b9d1-46292b2c2e58",
   "metadata": {},
   "source": [
    "# MERGE SCD 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2423fea2-05fe-4cc3-99d7-5a2986771c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando o MERGE (Upsert) entre employe e employe_update usando Spark SQL\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO employe AS e\n",
    "USING employe_update AS u\n",
    "ON e.emp_id = u.emp_id\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET e.salary = u.salary\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c09c5e-7b14-4b5d-8b3e-b622f42380c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+\n",
      "|emp_id|emp_name|dept_code| salary|\n",
      "+------+--------+---------+-------+\n",
      "|  1001|   Alice|     D101|58000.0|\n",
      "|  1002|     Bob|     D102|62000.0|\n",
      "|  1006|   Frank|     D106|70000.0|\n",
      "|  1007|   Grace|     D107|75000.0|\n",
      "|  1003| Charlie|     D103|75000.0|\n",
      "|  1004|   David|     D104|65000.0|\n",
      "|  1005|     Eve|     D105|70000.0|\n",
      "+------+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employe\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add43dd3-6cd1-430b-bd03-48b1ecf60abe",
   "metadata": {},
   "source": [
    "# Encerramento Sessão SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584ee11e-626d-490e-b17b-8d4e8a4b9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
